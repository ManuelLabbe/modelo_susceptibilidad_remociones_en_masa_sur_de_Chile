{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util import comparar_columnas, eliminar_caracteristicas, prueba_umbral\n",
    "from feature_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('final_dataframe.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = [\n",
    "    'alpha.0-5cm', 'ksat_0-5cm', 'PIRange_Sand.0-5cm', 'alpha_0-5cm', \n",
    "    'ksat_5-15cm', 'PIRange_Sand.15-30cm', 'alpha_5-15cm', 'ksat_15-30cm', \n",
    "    'PIRange_Sand.60-100cm', 'alpha_15-30cm', 'ksat_60-100cm', 'PP', \n",
    "    'AvMoist.0-5cm', 'ksat_100-200cm', 'PWP.0-5cm', 'AvMoist.5-15cm', \n",
    "    'n_0-5cm', 'PWP.100-200cm', 'AvMoist.60-100cm', 'n_5-15cm', 'slope', \n",
    "    'Bulkd.0-5cm', 'n_15-30cm', 'Tex_Class.0-5cm', 'Bulkd.5-15cm', \n",
    "    'n_60-100cm', 'Tex_Class.5-15cm', 'Bulkd.60-100cm', 'n_100-200cm', \n",
    "    'Tex_Class.15-30cm', 'Clay.0-5cm', 'PIRange_Bulkd.0-5cm', \n",
    "    'Tex_Class.100-200cm', 'Clay.5-15cm', 'PIRange_Bulkd.5-15cm', \n",
    "    'theta_s_0-5cm', 'Clay.15-30cm', 'PIRange_Bulkd.60-100cm', \n",
    "    'theta_s_5-15cm', 'FC.0-5cm', 'PIRange_Clay.0-5cm', 'theta_s_15-30cm', \n",
    "    'FC.5-15cm', 'PIRange_Clay.5-15cm', 'theta_s_60-100cm', 'FC.60-100cm', \n",
    "    'PIRange_Clay.15-30cm', 'VMoist', 'Valor'\n",
    "]\n",
    "corr_features[47] = 'valor_humedad_suelo1'\n",
    "final.columns = final.columns.str.replace('.tif', '')\n",
    "features = ['PP',\n",
    "'n_5-15cm',\n",
    "'slope',\n",
    "'Bulkd.5-15cm',\n",
    "'n_100-200cm',\n",
    "'Tex_Class.100-200cm',\n",
    "'theta_s_0-5cm',\n",
    "'theta_s_15-30cm',\n",
    "'valor_humedad_suelo1',\n",
    "'Valor']\n",
    "final = final[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 151200 candidates, totalling 453600 fits\n",
      "Mejores parámetros: {'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_jobs': -1, 'random_state': 42}\n",
      "Error en test con intervalo de confianza del 95%: 0.1095 ± 0.0244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Grid de hiperparámetros extendido para RandomForest\n",
    "param_grid = {\n",
    "    # Parámetros relacionados con la estructura de los árboles\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    \n",
    "    # Parámetros de aleatoriedad y features\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [42],\n",
    "    \n",
    "    # Parámetros de regularización y peso\n",
    "    'max_leaf_nodes': [None, 50, 100, 200],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05],\n",
    "    \n",
    "    # Parámetros de paralelización\n",
    "    'n_jobs': [-1]  # Usa todos los cores disponibles\n",
    "}\n",
    "\n",
    "def train_rf_with_confidence(X_train, X_test, y_train, y_test, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Entrena un RandomForest usando GridSearchCV y calcula el intervalo de confianza\n",
    "    del error (1 - accuracy)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Datos de entrenamiento\n",
    "    X_test : array-like\n",
    "        Datos de prueba\n",
    "    y_train : array-like\n",
    "        Etiquetas de entrenamiento\n",
    "    y_test : array-like\n",
    "        Etiquetas de prueba\n",
    "    confidence_level : float, optional (default=0.95)\n",
    "        Nivel de confianza para el intervalo\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Diccionario con los mejores parámetros, error y su intervalo de confianza\n",
    "    \"\"\"\n",
    "    # Crear el modelo base\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    # Configurar la búsqueda de grid con validación cruzada\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Ajustar el modelo\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtener predicciones en el conjunto de prueba\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "    # Calcular el error (1 - accuracy) para cada predicción\n",
    "    errors = y_pred != y_test\n",
    "    error_rate = np.mean(errors)\n",
    "    \n",
    "    # Calcular el intervalo de confianza\n",
    "    n = len(y_test)\n",
    "    z = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    margin_of_error = z * np.sqrt((error_rate * (1 - error_rate)) / n)\n",
    "    \n",
    "    results = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'test_error': error_rate,\n",
    "        'confidence_interval': margin_of_error,\n",
    "        'error_range': f\"{error_rate:.4f} ± {margin_of_error:.4f}\"\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "target_column = 'Valor'\n",
    "X = final.drop(target_column, axis=1)\n",
    "y = final[target_column]\n",
    "    \n",
    "    # Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "results = train_rf_with_confidence(X_train, X_test, y_train, y_test)\n",
    "print(\"Mejores parámetros:\", results['best_params'])\n",
    "print(f\"Error en test con intervalo de confianza del 95%: {results['error_range']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ivo\\.conda\\envs\\geotiff\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:36:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 500, 'random_state': 42}\n",
      "Error en test con intervalo de confianza del 95%: 0.1032 ± 0.0238\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Grid de hiperparámetros extendido para XGBoost\n",
    "param_grid = {\n",
    "    # Parámetros de estructura del árbol\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    #'min_child_weight': [1, 3, 5, 7],\n",
    "    #'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \n",
    "    # Parámetros de regulación\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    #'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    #'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \n",
    "    # Parámetros de regularización\n",
    "    #'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    #'reg_lambda': [0.1, 0.5, 1.0, 2.0],\n",
    "    \n",
    "    \n",
    "    # Otros parámetros\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "def train_xgb_with_confidence(X_train, X_test, y_train, y_test, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Entrena un XGBoost usando GridSearchCV y calcula el intervalo de confianza\n",
    "    del error (1 - accuracy)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Datos de entrenamiento\n",
    "    X_test : array-like\n",
    "        Datos de prueba\n",
    "    y_train : array-like\n",
    "        Etiquetas de entrenamiento\n",
    "    y_test : array-like\n",
    "        Etiquetas de prueba\n",
    "    confidence_level : float, optional (default=0.95)\n",
    "        Nivel de confianza para el intervalo\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Diccionario con los mejores parámetros, error y su intervalo de confianza\n",
    "    \"\"\"\n",
    "    # Crear el modelo base\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, verbosity=2)\n",
    "    \n",
    "    # Configurar la búsqueda de grid con validación cruzada\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        verbose=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Ajustar el modelo\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtener predicciones en el conjunto de prueba\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "    # Calcular el error (1 - accuracy) para cada predicción\n",
    "    errors = y_pred != y_test\n",
    "    error_rate = np.mean(errors)\n",
    "    \n",
    "    # Calcular el intervalo de confianza\n",
    "    n = len(y_test)\n",
    "    z = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    margin_of_error = z * np.sqrt((error_rate * (1 - error_rate)) / n)\n",
    "    \n",
    "    results = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'test_error': error_rate,\n",
    "        'confidence_interval': margin_of_error,\n",
    "        'error_range': f\"{error_rate:.4f} ± {margin_of_error:.4f}\"\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "target_column = 'Valor'\n",
    "X = final.drop(target_column, axis=1)\n",
    "y = final[target_column]\n",
    "    \n",
    "    # Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "results = train_xgb_with_confidence(X_train, X_test, y_train, y_test)\n",
    "print(\"Mejores parámetros:\", results['best_params'])\n",
    "print(f\"Error en test con intervalo de confianza del 95%: {results['error_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Grid de hiperparámetros extendido para SVC\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],  # Parámetro de penalización\n",
    "    'kernel': ['rbf'],  # Tipos de kernel\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1,10,20,30,50,100],  # Parámetro del kernel RBF\n",
    "    'random_state': [42],  # Reproducibilidad\n",
    "}\n",
    "\n",
    "def train_svc_with_confidence(X_train, X_test, y_train, y_test, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Entrena un modelo SVC usando GridSearchCV y calcula el intervalo de confianza\n",
    "    del error (1 - accuracy)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------- \n",
    "    X_train : array-like\n",
    "        Datos de entrenamiento\n",
    "    X_test : array-like\n",
    "        Datos de prueba\n",
    "    y_train : array-like\n",
    "        Etiquetas de entrenamiento\n",
    "    y_test : array-like\n",
    "        Etiquetas de prueba\n",
    "    confidence_level : float, optional (default=0.95)\n",
    "        Nivel de confianza para el intervalo\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Diccionario con los mejores parámetros, error y su intervalo de confianza\n",
    "    \"\"\"\n",
    "    # Crear el modelo base\n",
    "    svc = SVC()\n",
    "    \n",
    "    # Configurar la búsqueda de grid con validación cruzada\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=svc,\n",
    "        param_grid=param_grid_svc,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Ajustar el modelo\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtener predicciones en el conjunto de prueba\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "    # Calcular el error (1 - accuracy) para cada predicción\n",
    "    errors = y_pred != y_test\n",
    "    error_rate = np.mean(errors)\n",
    "    \n",
    "    # Calcular el intervalo de confianza\n",
    "    n = len(y_test)\n",
    "    z = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    margin_of_error = z * np.sqrt((error_rate * (1 - error_rate)) / n)\n",
    "    \n",
    "    results = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'test_error': error_rate,\n",
    "        'confidence_interval': margin_of_error,\n",
    "        'error_range': f\"{error_rate:.4f} ± {margin_of_error:.4f}\"\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "target_column = 'Valor'\n",
    "X = final.drop(target_column, axis=1)\n",
    "y = final[target_column]\n",
    "\n",
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "results = train_svc_with_confidence(X_train, X_test, y_train, y_test)\n",
    "print(\"Mejores parámetros:\", results['best_params'])\n",
    "print(f\"Error en test con intervalo de confianza del 95%: {results['error_range']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
